{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is (are) Glaucoma ?</td>\n",
       "      <td>Glaucoma is a group of diseases that can damag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is (are) Glaucoma ?</td>\n",
       "      <td>The optic nerve is a bundle of more than 1 mil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is (are) Glaucoma ?</td>\n",
       "      <td>Open-angle glaucoma is the most common form of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who is at risk for Glaucoma? ?</td>\n",
       "      <td>Anyone can develop glaucoma. Some people are a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How to prevent Glaucoma ?</td>\n",
       "      <td>At this time, we do not know how to prevent gl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         question  \\\n",
       "0        What is (are) Glaucoma ?   \n",
       "1        What is (are) Glaucoma ?   \n",
       "2        What is (are) Glaucoma ?   \n",
       "3  Who is at risk for Glaucoma? ?   \n",
       "4       How to prevent Glaucoma ?   \n",
       "\n",
       "                                              answer  \n",
       "0  Glaucoma is a group of diseases that can damag...  \n",
       "1  The optic nerve is a bundle of more than 1 mil...  \n",
       "2  Open-angle glaucoma is the most common form of...  \n",
       "3  Anyone can develop glaucoma. Some people are a...  \n",
       "4  At this time, we do not know how to prevent gl...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"intern_screening_dataset.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since there are more than one rows for the same question so group by is performed on the question column and sum aggregation is performed so that the answers could be in one place for the same question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.groupby(\"question\").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The simplest model is made based on the cosine similarity, So that the best matched question can be known and the respective answer can be shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: What is A1C\n",
      "Answer: Summary : A1C is a blood test for type 2 diabetes and prediabetes. It measures your average blood glucose, or blood sugar, level over the past 3 months. Doctors may use the A1C alone or in combination with other diabetes tests to make a diagnosis. They also use the A1C to see how well you are managing your diabetes. This test is different from the blood sugar checks that people with diabetes do every day.    Your A1C test result is given in percentages. The higher the percentage, the higher your blood sugar levels have been:       - A normal A1C level is below 5.7 percent    - Prediabetes is between 5.7 to 6.4 percent. Having prediabetes is a risk factor for getting type 2 diabetes. People with prediabetes may need retests every year.    - Type 2 diabetes is above 6.5 percent    - If you have diabetes, you should have the A1C test at least twice a year. The A1C goal for many people with diabetes is below 7. It may be different for you. Ask what your goal should be. If your A1C result is too high, you may need to change your diabetes care plan.       NIH: National Institute of Diabetes and Digestive and Kidney Diseases\n",
      "Question 2: Do you have information about Angioplasty\n",
      "Answer: Summary : If you have coronary artery disease, the arteries in your heart are narrowed or blocked by a sticky material called plaque. Angioplasty is a procedure to restore blood flow through the artery.    You have angioplasty in a hospital. The doctor threads a thin tube through a blood vessel in the arm or groin up to the involved site in the artery. The tube has a tiny balloon on the end. When the tube is in place, the doctor inflates the balloon to push the plaque outward against the wall of the artery. This widens the artery and restores blood flow.    Doctors may use angioplasty to       - Reduce chest pain caused by reduced blood flow to the heart    - Minimize damage to heart muscle from a heart attack       Many people go home the day after angioplasty, and are able to return to work within a week of coming home.    NIH: National Heart, Lung, and Blood Institute\n",
      "Question 1: Tell me something about Alcohol\n",
      "Answer: Summary : If you are like many Americans, you drink alcohol at least occasionally. For many people, moderate drinking is probably safe. It may even have health benefits, including reducing your risk of certain heart problems. For most women and for most people over 65, moderate drinking is no more than three drinks a day or seven drinks per week. For men under 65, it is no more than four drinks a day or 14 drinks per week.    Some people should not drink at all, including alcoholics, children, pregnant women, people taking certain medicines, and people with certain medical conditions. If you have questions about whether it is safe for you to drink, speak with your health care provider.    Anything more than moderate drinking can be risky. Heavy drinking can lead to alcoholism and alcohol abuse, as well as injuries, liver disease, heart disease, cancer, and other health problems. It can also cause problems at home, at work, and with friends.    NIH: National Institute on Alcohol Abuse and Alcoholism\n",
      "Question 4: Anatomy\n",
      "Answer: Summary : Anatomy is the science that studies the structure of the body. On this page, you'll find links to descriptions and pictures of the human body's parts and organ systems from head to toe.\n",
      "Question 5: Summary about Wilson disease\n",
      "Answer: Wilson disease is a rare inherited disorder that prevents your body from getting rid of extra copper. You need a small amount of copper from food to stay healthy. Too much copper is poisonous.    Normally, your liver releases extra copper into bile, a digestive fluid. With Wilson disease, the copper builds up in your liver, and it releases the copper directly into your bloodstream. This can cause damage to your brain, kidneys, and eyes.    Wilson disease is present at birth, but symptoms usually start between ages 5 and 35. It first attacks the liver, the central nervous system or both. The most characteristic sign is a rusty brown ring around the cornea of the eye. A physical exam and laboratory tests can diagnose it.    Treatment is with drugs to remove the extra copper from your body. You need to take medicine and follow a low-copper diet for the rest of your life. Don't eat shellfish or liver, as these foods may contain high levels of copper. At the beginning of treatment, you'll also need to avoid chocolate, mushrooms, and nuts. Have your drinking water checked for copper content and don't take multivitamins that contain copper.    With early detection and proper treatment, you can enjoy good health.    NIH: National Institute of Diabetes and Digestive and Kidney DiseasesWilson disease (WD) is a rare inherited disorder of copper metabolism in which excessive amounts of copper accumulate in the body. The buildup of copper leads to damage in the liver, brain, and eyes. Although copper accumulation begins at birth, symptoms of the disorder only appear later in life. The most characteristic sign of WD is the Kayser-Fleisher ring  a rusty brown ring around the cornea of the eye that can best be viewed using an ophthalmologists slit lamp. The primary consequence for most individuals with WD is liver disease, appearing in late childhood or early adolescence as acute hepatitis, liver failure, or progressive chronic liver disease in the form of chronic active hepatitis or cirrhosis of the liver. In others, the first symptoms are neurological, occur later in adulthood, and commonly include slurred speech (dysarthria), difficulty swallowing (dysphagia), and drooling. Other symptoms may include tremor of the head, arms, or legs; impaired muscle tone, and sustained muscle contractions that produce abnormal postures, twisting, and repetitive movements (dystonia); and slowness of movements (bradykinesia). Individuals may also experience clumsiness (ataxia) and loss of fine motor skills. One-third of individuals with WD will also experience psychiatric symptoms such as an abrupt personality change, bizarre and inappropriate behavior, depression accompanied by suicidal thoughts, neurosis, or psychosis. WD is diagnosed with tests that measure the amount of copper in the blood, urine, and liver.Wilson disease is a genetic disease that prevents the body from removing extra copper. The body needs a small amount of copper from food to stay healthy; however, too much copper is poisonous. Normally, the liver filters extra copper and releases it into bile. Bile is a fluid made by the liver that carries toxins and wastes out of the body through the gastrointestinal tract. In Wilson disease, the liver does not filter copper correctly and copper builds up in the liver, brain, eyes, and other organs. Over time, high copper levels can cause life-threatening organ damage.The liver is the bodys largest internal organ. The liver is called the bodys metabolic factory because of the important role it plays in metabolismthe way cells change food into energy after food is digested and absorbed into the blood. The liver has many important functions, including\n",
      "                \n",
      "- taking up, storing, and processing nutrients from foodincluding fat, sugar, and proteinand delivering them to the rest of the body when needed.  - making new proteins, such as clotting factors and immune factors.  - producing bile. In addition to carrying toxins and waste products out of the body, bile helps the body digest fats and the fat-soluble vitamins A, D, E, and K.  - removing waste products the kidneys cannot remove, such as fats, cholesterol, toxins, and medications.\n",
      "                \n",
      "A healthy liver is necessary for survival. The liver can regenerate most of its own cells when they become damaged. However, if injury to the liver is too severe or long lasting, regeneration is incomplete and the liver creates scar tissue.\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "questions = list(data.index)\n",
    "answers = data.answer\n",
    "\n",
    "# Preprocess the dataset\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "question_vectors = tfidf_vectorizer.fit_transform(questions)\n",
    "\n",
    "def get_answer(input_question):\n",
    "    # Preprocess input question\n",
    "    input_question_vector = tfidf_vectorizer.transform([input_question])\n",
    "    \n",
    "    # Calculate cosine similarity between input question and dataset questions\n",
    "    similarities = cosine_similarity(input_question_vector, question_vectors)\n",
    "    \n",
    "    # Find the index of the most similar question\n",
    "    most_similar_index = similarities.argmax()\n",
    "    \n",
    "    # Return the corresponding answer\n",
    "    return answers[most_similar_index]\n",
    "\n",
    "# Test the QA system\n",
    "input_question = \"What is A1C\"\n",
    "print(\"Question 1:\", input_question)\n",
    "print(\"Answer:\", get_answer(input_question))\n",
    "\n",
    "input_question = \"Do you have information about Angioplasty\"\n",
    "print(\"Question 2:\", input_question)\n",
    "print(\"Answer:\", get_answer(input_question))\n",
    "\n",
    "input_question = \"Tell me something about Alcohol\"\n",
    "print(\"Question 1:\", input_question)\n",
    "print(\"Answer:\", get_answer(input_question))\n",
    "\n",
    "input_question = \"Anatomy\"\n",
    "print(\"Question 4:\", input_question)\n",
    "print(\"Answer:\", get_answer(input_question))\n",
    "\n",
    "input_question = \"Summary about Wilson disease\"\n",
    "print(\"Question 5:\", input_question)\n",
    "print(\"Answer:\", get_answer(input_question))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A more accurate model can be made using transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = ''\n",
    "for i in range(len(data)):\n",
    "    q = str(data.index[i])\n",
    "    a = str(data.answer[i])\n",
    "    txt += q + a\n",
    "\n",
    "# Question and its answer is placed sequencially one after another in a string so that the transformer model can understand the sequence of\n",
    "# question and answer and also the context of answer to a question and we can build a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# here are all the unique words that occur in this text\n",
    "words = sorted(list(set(txt.split())))\n",
    "vocab_size = len(words)\n",
    "# create a mapping from words to integers\n",
    "stoi = { ch:i for i,ch in enumerate(words) }\n",
    "itos = { i:ch for i,ch in enumerate(words) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ' '.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(txt.split()), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
